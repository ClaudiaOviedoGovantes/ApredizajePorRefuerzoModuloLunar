{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduccion al problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows:\n",
    "\n",
    "method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ufal.pybox2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lunar import LunarLanderEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow or Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miniluz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\aprendizaje-modulo-lunar-5laruUkD-py3.10\\lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.discrete.Discrete'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "lunar = LunarLanderEnv()\n",
    "print(type(lunar.env.observation_space))\n",
    "print(type(lunar.env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de acciones es un valor del 0 al 3 que indica que acciones tomará el modulo lunar para esa iteración.\n",
    "\n",
    "en concreto son las siguientes:\n",
    "\n",
    "|value| action                        |\n",
    "|-----|-------------------------------|\n",
    "| 0   | do nothing                    |\n",
    "| 1   | fire left orientation engine  |\n",
    "| 2   | fire main engine              |\n",
    "| 3   | fire right orientation engine |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar.env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de observaciones son un conjunto de valores flotantes y booleanos que indica el estado del modulo lunar.\n",
    "\n",
    "en concreto son las siguientes:\n",
    "\n",
    "|value| observation                               |\n",
    "|-----|-------------------------------------------|\n",
    "| 0   | coordenada X (float)                      |\n",
    "| 1   | coordenada Y (float)                      |\n",
    "| 2   | velocidad lineal X (float)                |\n",
    "| 3   | velocidad lineal Y (float)                |\n",
    "| 4   | Angulo en radianes desde -2π a +2π (float)|\n",
    "| 5   | Velocidad angula (float)                  |\n",
    "| 6   | Contacto de la pierna Izquierda (bool)    |\n",
    "| 7   | Contacto de la pierna Derecha (bool)      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se muestran los valores minimos y maximos del espacio de observaciones.\n",
    "lunar.env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_count = lunar.env.observation_space.shape[0] \n",
    "action_count = lunar.env.action_space.n\n",
    "\n",
    "print(f\"observations: {observation_count}, actions: {action_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valores minimos y maximos para las observaciones.\n",
    "print(lunar.env.observation_space.low) \n",
    "print(lunar.env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample ofrece una combinacion aleatoria del conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lunar.env.action_space.sample())  # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lunar.env.observation_space.sample())  # Sample a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a random episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lunar_lander(steps_to_run_before_pause, agent, episodes=1, render_mode=\"human\"):\n",
    "    \"\"\"\n",
    "    Test the Lunar Lander environment with a given agent.\n",
    "    \n",
    "    Parameters:\n",
    "    steps_to_run_before_pause (int): Number of steps to run before pausing for user input.\n",
    "    agent: The agent to be tested in the environment.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Initialize the environment\n",
    "    lunar = LunarLanderEnv(render_mode=render_mode)\n",
    "    \n",
    "    if(agent is not None):\n",
    "        # Set the agent's environment\n",
    "        agent.lunar = lunar\n",
    "        \n",
    "    for _ in range(episodes):\n",
    "        counter, score = 0, 0\n",
    "\n",
    "        while True:\n",
    "            if steps_to_run_before_pause != 0 and counter % steps_to_run_before_pause == 0:\n",
    "                input(\"Press Enter to continue...\")\n",
    "\n",
    "            if(agent is not None):\n",
    "                _, reward, done, action = agent.act()\n",
    "                \n",
    "            else:\n",
    "                # Sample a random action from the action space\n",
    "                action = lunar.env.action_space.sample()\n",
    "            \n",
    "                # Take a step in the environment\n",
    "                _, reward, done = lunar.take_action(action, verbose=True)\n",
    "                \n",
    "            score += reward\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Episode finished, score: {score}\")\n",
    "                break\n",
    "        if(agent is not None):\n",
    "            # Reset the agent's environment for the next episode\n",
    "            agent.lunar.reset()\n",
    "        else:\n",
    "            # Reset the environment for the next episode\n",
    "            lunar.reset()\n",
    "        \n",
    "    # Close the environment\n",
    "    lunar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step taken: 3, New state: [-0.00748186  1.4302709  -0.37366655  0.41704333  0.00706639  0.05377426\n",
      "  0.          0.        ], Reward: 1.2662842594675123, Done: False\n",
      "Step taken: 0, New state: [-0.01119146  1.4390559  -0.37367526  0.39042535  0.00975317  0.05374061\n",
      "  0.          0.        ], Reward: 0.8030677881830002, Done: False\n",
      "Step taken: 1, New state: [-0.01499109  1.4472294  -0.38497022  0.36322883  0.01470458  0.09903754\n",
      "  0.          0.        ], Reward: -0.23084533488045508, Done: False\n",
      "Step taken: 0, New state: [-0.01879091  1.4548032  -0.3849855   0.33655885  0.01965423  0.09900191\n",
      "  0.          0.        ], Reward: 0.5356000611947707, Done: False\n",
      "Step taken: 1, New state: [-0.02267628  1.4617712  -0.39571527  0.30957502  0.02675408  0.14201018\n",
      "  0.          0.        ], Reward: -0.5487068002751119, Done: False\n",
      "Step taken: 2, New state: [-0.02662649  1.4694358  -0.40199605  0.34050336  0.03365359  0.13800322\n",
      "  0.          0.        ], Reward: -4.20318398846918, Done: False\n",
      "Step taken: 2, New state: [-0.03042459  1.4773419  -0.38760856  0.35117987  0.04136765  0.15429564\n",
      "  0.          0.        ], Reward: -1.4905230197666015, Done: False\n",
      "Step taken: 0, New state: [-0.03422289  1.4846485  -0.3876312   0.32450512  0.04907978  0.15425675\n",
      "  0.          0.        ], Reward: 0.24061671013217278, Done: False\n",
      "Step taken: 1, New state: [-0.03809214  1.491346   -0.39651224  0.2973312   0.05857418  0.18990567\n",
      "  0.          0.        ], Reward: -0.6661517504472261, Done: False\n",
      "Step taken: 2, New state: [-0.04200726  1.498699   -0.40110525  0.3263963   0.06808274  0.19018853\n",
      "  0.          0.        ], Reward: -4.148185202696442, Done: False\n",
      "Step taken: 3, New state: [-0.04583035  1.5054586  -0.38955775  0.3000821   0.07526617  0.14368184\n",
      "  0.          0.        ], Reward: 1.10385671240016, Done: False\n",
      "Step taken: 2, New state: [-0.04978228  1.5123394  -0.40198833  0.30544803  0.08201429  0.13497458\n",
      "  0.          0.        ], Reward: -2.9883758042391717, Done: False\n",
      "Step taken: 0, New state: [-0.05373421  1.5186204  -0.4020068   0.27876937  0.08876124  0.13495153\n",
      "  0.          0.        ], Reward: 0.2504810519615148, Done: False\n",
      "Step taken: 1, New state: [-0.05776367  1.5243021  -0.41168195  0.25198114  0.09743851  0.17356145\n",
      "  0.          0.        ], Reward: -0.8273806985386887, Done: False\n",
      "Step taken: 0, New state: [-0.06179342  1.5293846  -0.41170734  0.22530533  0.10611364  0.17351823\n",
      "  0.          0.        ], Reward: -0.05593804162873539, Done: False\n",
      "Step taken: 3, New state: [-0.06574259  1.5338894  -0.4015573   0.19972494  0.11271647  0.13206856\n",
      "  0.          0.        ], Reward: 0.927194965606476, Done: False\n",
      "Step taken: 1, New state: [-0.06977854  1.5377699  -0.412465    0.17178363  0.12154156  0.17651793\n",
      "  0.          0.        ], Reward: -1.1502841712544114, Done: False\n",
      "Step taken: 3, New state: [-0.0737402   1.5410537  -0.40314323  0.14536309  0.12849328  0.13904713\n",
      "  0.          0.        ], Reward: 0.7541445963223634, Done: False\n",
      "Step taken: 3, New state: [-0.07760477  1.5437541  -0.39094818  0.11962334  0.13297303  0.08960257\n",
      "  0.          0.        ], Reward: 1.2043394108415282, Done: False\n",
      "Step taken: 0, New state: [-0.08146954  1.5458548  -0.39095974  0.09295191  0.13745272  0.08960204\n",
      "  0.          0.        ], Reward: 0.020609775759140803, Done: False\n",
      "Step taken: 3, New state: [-0.08525448  1.5473793  -0.38092205  0.06753448  0.13987176  0.04838502\n",
      "  0.          0.        ], Reward: 1.0550218078885234, Done: False\n",
      "Step taken: 2, New state: [-0.08923616  1.5493114  -0.4001275   0.08569169  0.1418205   0.03897886\n",
      "  0.          0.        ], Reward: -2.944002109433046, Done: False\n",
      "Step taken: 2, New state: [-0.09320135  1.5521858  -0.3992291   0.12748817  0.1445192   0.05397894\n",
      "  0.          0.        ], Reward: -1.8691152731212697, Done: False\n",
      "Step taken: 3, New state: [-0.09707041  1.5544927  -0.38710415  0.10251342  0.14471926  0.00400191\n",
      "  0.          0.        ], Reward: 1.5603519833146617, Done: False\n",
      "Step taken: 1, New state: [-0.10099812  1.5561634  -0.39453498  0.07408022  0.14647894  0.03519348\n",
      "  0.          0.        ], Reward: -0.4958147366743322, Done: False\n",
      "Step taken: 3, New state: [-1.0486040e-01  1.5572532e+00 -3.8629603e-01  4.8426013e-02\n",
      "  1.4655042e-01  1.4293162e-03  0.0000000e+00  0.0000000e+00], Reward: 1.0396370120257654, Done: False\n",
      "Step taken: 0, New state: [-1.0872259e-01  1.5577431e+00 -3.8629603e-01  2.1759348e-02\n",
      "  1.4662188e-01  1.4293076e-03  0.0000000e+00  0.0000000e+00], Reward: 0.158683964234001, Done: False\n",
      "Step taken: 3, New state: [-0.11252518  1.5576373  -0.37881827 -0.00455259  0.14519241 -0.02858945\n",
      "  0.          0.        ], Reward: 0.902829626994021, Done: False\n",
      "Step taken: 0, New state: [-0.11632767  1.556932   -0.3788182  -0.03121943  0.14376295 -0.02858947\n",
      "  0.          0.        ], Reward: 0.05974493155389382, Done: False\n",
      "Step taken: 0, New state: [-0.12013016  1.5556263  -0.3788182  -0.05788626  0.1423335  -0.02858948\n",
      "  0.          0.        ], Reward: -0.06696328747992197, Done: False\n",
      "Step taken: 3, New state: [-0.12384291  1.5537424  -0.36752713 -0.08337196  0.13860235 -0.07462321\n",
      "  0.          0.        ], Reward: 1.136940429281908, Done: False\n",
      "Step taken: 3, New state: [-0.12747765  1.5512805  -0.3577197  -0.10889407  0.1328661  -0.11472495\n",
      "  0.          0.        ], Reward: 1.053483516045021, Done: False\n",
      "Step taken: 0, New state: [-0.13111258  1.5482191  -0.35771912 -0.13556346  0.12712988 -0.1147247\n",
      "  0.          0.        ], Reward: -0.013298823025621687, Done: False\n",
      "Step taken: 0, New state: [-0.13474742  1.544558   -0.3577186  -0.16223286  0.12139367 -0.11472443\n",
      "  0.          0.        ], Reward: -0.11703499197938072, Done: False\n",
      "Step taken: 2, New state: [-0.13863364  1.5411642  -0.38201797 -0.150316    0.11482651 -0.13134325\n",
      "  0.          0.        ], Reward: -1.1134820161027108, Done: False\n",
      "Step taken: 1, New state: [-0.14260206  1.5371498  -0.39234942 -0.17807667  0.11036218 -0.08928673\n",
      "  0.          0.        ], Reward: -1.2542283963260548, Done: False\n",
      "Step taken: 2, New state: [-0.14650345  1.5336299  -0.3863286  -0.15615916  0.10657746 -0.0756942\n",
      "  0.          0.        ], Reward: 1.8098011216612064, Done: False\n",
      "Step taken: 2, New state: [-0.15057945  1.5309298  -0.4034554  -0.11972069  0.1024543  -0.08246318\n",
      "  0.          0.        ], Reward: -0.07302967721914227, Done: False\n",
      "Step taken: 2, New state: [-0.15474682  1.5284735  -0.4124182  -0.10887586  0.09817736 -0.08553862\n",
      "  0.          0.        ], Reward: -0.23965775417981944, Done: False\n",
      "Step taken: 1, New state: [-0.15898399  1.5254099  -0.42119384 -0.13599628  0.09566621 -0.05022296\n",
      "  0.          0.        ], Reward: -1.123195740808966, Done: False\n",
      "Step taken: 1, New state: [-0.16328907  1.5217392  -0.42970735 -0.16308816  0.09486755 -0.01597297\n",
      "  0.          0.        ], Reward: -1.3313956722556657, Done: False\n",
      "Step taken: 1, New state: [-0.16766386  1.5174495  -0.4384756  -0.19071603  0.09585147  0.01967829\n",
      "  0.          0.        ], Reward: -1.603415342759887, Done: False\n",
      "Step taken: 1, New state: [-0.17213202  1.5125427  -0.4501904  -0.21830104  0.09920392  0.06704903\n",
      "  0.          0.        ], Reward: -2.1444505870018973, Done: False\n",
      "Step taken: 0, New state: [-0.17660017  1.5070361  -0.45019025 -0.24496865  0.10255638  0.06704898\n",
      "  0.          0.        ], Reward: -1.0592255297715383, Done: False\n",
      "Step taken: 3, New state: [-0.18099098  1.5009304  -0.44051033 -0.27145875  0.10397371  0.02834669\n",
      "  0.          0.        ], Reward: -0.10838125795569795, Done: False\n",
      "Step taken: 3, New state: [-0.18529177  1.4942472  -0.4291942  -0.29697555  0.10309188 -0.01763653\n",
      "  0.          0.        ], Reward: 0.2207167179178964, Done: False\n",
      "Step taken: 0, New state: [-0.18959256  1.4869637  -0.42919415 -0.3236423   0.10221003 -0.01763682\n",
      "  0.          0.        ], Reward: -0.8049161445696598, Done: False\n",
      "Step taken: 3, New state: [-0.19382687  1.4790869  -0.42084628 -0.3499108   0.0996498  -0.05120456\n",
      "  0.          0.        ], Reward: -0.023910584057715595, Done: False\n",
      "Step taken: 2, New state: [-0.19815627  1.4720757  -0.4303785  -0.3114383   0.0971004  -0.05098795\n",
      "  0.          0.        ], Reward: 2.199706036045666, Done: False\n",
      "Step taken: 3, New state: [-0.20242496  1.4644849  -0.4227163  -0.3371018   0.0929869  -0.08226987\n",
      "  0.          0.        ], Reward: 0.13284991800310422, Done: False\n",
      "Step taken: 2, New state: [-0.20671248  1.4570209  -0.42472157 -0.33149317  0.08899438 -0.07985024\n",
      "  0.          0.        ], Reward: 0.9689474266462923, Done: False\n",
      "Step taken: 1, New state: [-0.21107355  1.448957   -0.4339353  -0.35826698  0.08684327 -0.0430222\n",
      "  0.          0.        ], Reward: -1.473597338390191, Done: False\n",
      "Step taken: 3, New state: [-0.21536025  1.4403135  -0.42458653 -0.38392687  0.0827939  -0.08098757\n",
      "  0.          0.        ], Reward: 0.19676170189578215, Done: False\n",
      "Step taken: 0, New state: [-0.21964698  1.4310701  -0.42458636 -0.4105949   0.07874452 -0.0809875\n",
      "  0.          0.        ], Reward: -0.5671032380359406, Done: False\n",
      "Step taken: 1, New state: [-0.22402573  1.4212053  -0.43614173 -0.4383477   0.07703449 -0.03420067\n",
      "  0.          0.        ], Reward: -1.7229601860113621, Done: False\n",
      "Step taken: 2, New state: [-0.22838235  1.4115852  -0.43424273 -0.4274806   0.07562631 -0.02816353\n",
      "  0.          0.        ], Reward: 1.623095251232013, Done: False\n",
      "Step taken: 2, New state: [-0.23294087  1.4022522  -0.4536956  -0.41469496  0.07349329 -0.04266056\n",
      "  0.          0.        ], Reward: 0.22909751104786552, Done: False\n",
      "Step taken: 3, New state: [-0.23742971  1.3923275  -0.44495267 -0.44090825  0.06960149 -0.07783584\n",
      "  0.          0.        ], Reward: 0.08926750529147512, Done: False\n",
      "Step taken: 2, New state: [-0.24192452  1.3832207  -0.44582614 -0.40458754  0.06598575 -0.07231493\n",
      "  0.          0.        ], Reward: 3.31897410094997, Done: False\n",
      "Step taken: 3, New state: [-0.24633245  1.3735291  -0.43491483 -0.43049055  0.06017053 -0.11630452\n",
      "  0.          0.        ], Reward: 0.43873856834568525, Done: False\n",
      "Step taken: 2, New state: [-0.2507887   1.364156   -0.4396875  -0.41635117  0.05430235 -0.11736339\n",
      "  0.          0.        ], Reward: 1.7700580689309902, Done: False\n",
      "Step taken: 3, New state: [-0.2551542   1.3541902  -0.42829952 -0.44265366  0.04614947 -0.16305777\n",
      "  0.          0.        ], Reward: 0.6446692504962368, Done: False\n",
      "Step taken: 3, New state: [-0.2594481   1.3436366  -0.41931242 -0.46876827  0.03618904 -0.19920838\n",
      "  0.          0.        ], Reward: 0.6222214892354725, Done: False\n",
      "Step taken: 2, New state: [-0.26392365  1.3334258  -0.43672377 -0.453591    0.02549397 -0.2139016\n",
      "  0.          0.        ], Reward: 1.6137751518666732, Done: False\n",
      "Step taken: 0, New state: [-0.26839924  1.3226165  -0.4367233  -0.48026726  0.01479899 -0.21389993\n",
      "  0.          0.        ], Reward: 0.0933753872770069, Done: False\n",
      "Step taken: 2, New state: [-0.27288738  1.3118314  -0.43795174 -0.47926956  0.004074   -0.21449979\n",
      "  0.          0.        ], Reward: 1.7295783712243178, Done: False\n",
      "Step taken: 1, New state: [-0.27747375  1.3004355  -0.45026845 -0.5064831  -0.00418344 -0.16514876\n",
      "  0.          0.        ], Reward: -1.8665026301543424, Done: False\n",
      "Step taken: 2, New state: [-0.28217658  1.2899997  -0.46135584 -0.46385995 -0.01300073 -0.17634578\n",
      "  0.          0.        ], Reward: 2.085346405099938, Done: False\n",
      "Step taken: 2, New state: [-0.2870391   1.279936   -0.4765303  -0.44739515 -0.02259567 -0.19189873\n",
      "  0.          0.        ], Reward: -0.3230769468046219, Done: False\n",
      "Step taken: 1, New state: [-0.29199567  1.2692837  -0.48834077 -0.47356328 -0.02982162 -0.14451908\n",
      "  0.          0.        ], Reward: -2.4846605889088367, Done: False\n",
      "Step taken: 3, New state: [-0.29687914  1.2580242  -0.47916907 -0.5006254  -0.03888795 -0.18132666\n",
      "  0.          0.        ], Reward: -1.2245157980929366, Done: False\n",
      "Step taken: 0, New state: [-0.30176258  1.246166   -0.47916943 -0.5272989  -0.04795424 -0.18132564\n",
      "  0.          0.        ], Reward: -1.8176933525012942, Done: False\n",
      "Step taken: 1, New state: [-0.30674505  1.2337041  -0.4915828  -0.55409116 -0.05453963 -0.13170776\n",
      "  0.          0.        ], Reward: -2.419916536400278, Done: False\n",
      "Step taken: 0, New state: [-0.31172743  1.2206427  -0.49158305 -0.5807613  -0.061125   -0.13170718\n",
      "  0.          0.        ], Reward: -1.5293833226763809, Done: False\n",
      "Step taken: 1, New state: [-0.31678987  1.2069806  -0.501613   -0.60739875 -0.06570479 -0.09159583\n",
      "  0.          0.        ], Reward: -1.9792375686662058, Done: False\n",
      "Step taken: 2, New state: [-0.32185632  1.193625   -0.50180733 -0.5937996  -0.07048683 -0.09564095\n",
      "  0.          0.        ], Reward: 1.413412446563609, Done: False\n",
      "Step taken: 3, New state: [-0.32684702  1.1796694  -0.49232498 -0.62058    -0.07716611 -0.13358572\n",
      "  0.          0.        ], Reward: -0.9547513731291974, Done: False\n",
      "Step taken: 1, New state: [-0.3319294   1.1651231  -0.50382483 -0.6467388  -0.08153436 -0.08736493\n",
      "  0.          0.        ], Reward: -1.9711233300834852, Done: False\n",
      "Step taken: 0, New state: [-0.33701175  1.1499768  -0.50382495 -0.67340696 -0.08590258 -0.08736482\n",
      "  0.          0.        ], Reward: -1.2425796452218947, Done: False\n",
      "Step taken: 2, New state: [-0.34193498  1.1352599  -0.48837596 -0.65432245 -0.08980993 -0.07814703\n",
      "  0.          0.        ], Reward: 3.033335925601887, Done: False\n",
      "Step taken: 2, New state: [-0.34694242  1.12083    -0.49616376 -0.6416058  -0.09435128 -0.09082709\n",
      "  0.          0.        ], Reward: 1.021157204630424, Done: False\n",
      "Step taken: 3, New state: [-0.35188112  1.1057942  -0.48754162 -0.6686712  -0.10062582 -0.12549084\n",
      "  0.          0.        ], Reward: -1.017373105329084, Done: False\n",
      "Step taken: 2, New state: [-0.35659546  1.090783   -0.4659598  -0.66754955 -0.10606164 -0.10871635\n",
      "  0.          0.        ], Reward: 1.785167351216171, Done: False\n",
      "Step taken: 1, New state: [-0.36137152  1.0751925  -0.47372848 -0.6931972  -0.10990951 -0.07695727\n",
      "  0.          0.        ], Reward: -1.637057897242612, Done: False\n",
      "Step taken: 2, New state: [-0.36592     1.0601814  -0.45162788 -0.66740173 -0.1131106  -0.06402181\n",
      "  0.          0.        ], Reward: 4.03003538188558, Done: False\n",
      "Step taken: 1, New state: [-0.37055555  1.0445826  -0.4625513  -0.6933515  -0.11410753 -0.01993829\n",
      "  0.          0.        ], Reward: -1.5736141258206249, Done: False\n",
      "Step taken: 3, New state: [-0.3751034   1.0283695  -0.45154572 -0.7208316  -0.11732915 -0.0644324\n",
      "  0.          0.        ], Reward: -0.6907439208719086, Done: False\n",
      "Step taken: 2, New state: [-0.37962446  1.0126019  -0.44860727 -0.7010632  -0.1208053  -0.06952305\n",
      "  0.          0.        ], Reward: 2.501880927152155, Done: False\n",
      "Step taken: 0, New state: [-0.3841455   0.99623436 -0.44860744 -0.7277309  -0.12428145 -0.06952302\n",
      "  0.          0.        ], Reward: -1.2367412320234052, Done: False\n",
      "Step taken: 2, New state: [-0.38844067  0.97991925 -0.426812   -0.72534436 -0.12697035 -0.05377812\n",
      "  0.          0.        ], Reward: 2.1233096035773658, Done: False\n",
      "Step taken: 2, New state: [-0.39265543  0.964046   -0.41873223 -0.70571125 -0.12969753 -0.0545437\n",
      "  0.          0.        ], Reward: 2.8443174717005606, Done: False\n",
      "Step taken: 3, New state: [-0.3967828   0.9475445  -0.40773052 -0.7338458  -0.13468133 -0.09967601\n",
      "  0.          0.        ], Reward: -1.0527166396972507, Done: False\n",
      "Step taken: 3, New state: [-0.40082994  0.9304197  -0.3976425  -0.761757   -0.14173031 -0.14097996\n",
      "  0.          0.        ], Reward: -1.2959316927719715, Done: False\n",
      "Step taken: 1, New state: [-0.40494317  0.9126994  -0.40592137 -0.7880934  -0.14712037 -0.10780121\n",
      "  0.          0.        ], Reward: -1.8292586360286964, Done: False\n",
      "Step taken: 2, New state: [-0.40884703  0.8950111  -0.38565925 -0.786623   -0.15183815 -0.09435554\n",
      "  0.          0.        ], Reward: 1.722224647099307, Done: False\n",
      "Step taken: 0, New state: [-0.4127509   0.876723   -0.3856597  -0.81329155 -0.15655592 -0.09435528\n",
      "  0.          0.        ], Reward: -1.3791615876441767, Done: False\n",
      "Step taken: 3, New state: [-0.41656494  0.8578107  -0.3743739  -0.84130853 -0.16358578 -0.1405971\n",
      "  0.          0.        ], Reward: -1.2660027258920923, Done: False\n",
      "Step taken: 0, New state: [-0.42037892  0.8382989  -0.3743749  -0.8679792  -0.17061561 -0.1405966\n",
      "  0.          0.        ], Reward: -1.5650069183490984, Done: False\n",
      "Step taken: 3, New state: [-0.42411748  0.81815803 -0.36486888 -0.8962134  -0.17962216 -0.18013133\n",
      "  0.          0.        ], Reward: -1.5426909897435326, Done: False\n",
      "Step taken: 1, New state: [-0.42794186  0.7974388  -0.37566015 -0.92168677 -0.18642168 -0.13599072\n",
      "  0.          0.        ], Reward: -1.8219486356669268, Done: False\n",
      "Step taken: 1, New state: [-0.4318428   0.7761533  -0.385353   -0.94662297 -0.19119224 -0.09541132\n",
      "  0.          0.        ], Reward: -1.5012744737070978, Done: False\n",
      "Step taken: 1, New state: [-0.43580383  0.7542847  -0.39290625 -0.97236055 -0.194411   -0.06437467\n",
      "  0.          0.        ], Reward: -1.313817968722758, Done: False\n",
      "Step taken: 2, New state: [-0.43972096  0.73252934 -0.38828915 -0.96735644 -0.19785559 -0.06889191\n",
      "  0.          0.        ], Reward: 1.668041379903866, Done: False\n",
      "Step taken: 0, New state: [-0.44363803  0.71017426 -0.38828945 -0.99402416 -0.20130019 -0.06889188\n",
      "  0.          0.        ], Reward: -1.1220403775174645, Done: False\n",
      "Step taken: 0, New state: [-0.44755512  0.6872193  -0.38828975 -1.0206918  -0.20474479 -0.06889181\n",
      "  0.          0.        ], Reward: -1.1080584694211382, Done: False\n",
      "Step taken: 2, New state: [-0.45126373  0.6643577  -0.36794809 -1.0164738  -0.20768017 -0.05870747\n",
      "  0.          0.        ], Reward: 2.207988963665554, Done: False\n",
      "Step taken: 3, New state: [-0.45488778  0.640867   -0.35730636 -1.0447549  -0.21282673 -0.10293138\n",
      "  0.          0.        ], Reward: -1.1362771805858347, Done: False\n",
      "Step taken: 0, New state: [-0.45851177  0.61677676 -0.35730705 -1.0714236  -0.21797329 -0.1029312\n",
      "  0.          0.        ], Reward: -1.3052384710832712, Done: False\n",
      "Step taken: 1, New state: [-0.46221668  0.5921188  -0.36751342 -1.0963528  -0.22098215 -0.06017671\n",
      "  0.          0.        ], Reward: -1.2817606910314499, Done: False\n",
      "Step taken: 0, New state: [-0.4659215   0.5668609  -0.36751363 -1.1230202  -0.22399099 -0.06017669\n",
      "  0.          0.        ], Reward: -1.0925960320487036, Done: False\n",
      "Step taken: 0, New state: [-0.46962634  0.5410032  -0.3675139  -1.1496874  -0.22699983 -0.06017668\n",
      "  0.          0.        ], Reward: -1.1018040614133326, Done: False\n",
      "Step taken: 3, New state: [-0.47324786  0.5145096  -0.35699508 -1.178295   -0.23222634 -0.10453023\n",
      "  0.          0.        ], Reward: -1.2371425994410845, Done: False\n",
      "Step taken: 3, New state: [-0.47680783  0.48739395 -0.3492549  -1.206221   -0.2390708  -0.1368893\n",
      "  0.          0.        ], Reward: -1.449765759475041, Done: False\n",
      "Step taken: 1, New state: [-0.48046097  0.45972106 -0.3610352  -1.230608   -0.24341896 -0.08696304\n",
      "  0.          0.        ], Reward: -1.4493452197334011, Done: False\n",
      "Step taken: 3, New state: [-0.48402953  0.43142617 -0.35044155 -1.258623   -0.2499487  -0.13059497\n",
      "  0.          0.        ], Reward: -1.4276091481451647, Done: False\n",
      "Step taken: 1, New state: [-0.4876569   0.4025573  -0.3578696  -1.2838986  -0.2549069  -0.09916423\n",
      "  0.          0.        ], Reward: -1.555339273041766, Done: False\n",
      "Step taken: 2, New state: [-0.49107403  0.37403885 -0.33705816 -1.2683041  -0.25966388 -0.09513948\n",
      "  0.          0.        ], Reward: 2.780345605175836, Done: False\n",
      "Step taken: 2, New state: [-0.4944741   0.34597543 -0.33461004 -1.2482294  -0.26518017 -0.11032566\n",
      "  0.          0.        ], Reward: 2.531745496142361, Done: False\n",
      "Step taken: 2, New state: [-0.49766     0.31843838 -0.31324583 -1.224844   -0.27065128 -0.10942197\n",
      "  0.          0.        ], Reward: 3.223711556572778, Done: False\n",
      "Step taken: 1, New state: [-0.5009309   0.29034728 -0.3240207  -1.2490656  -0.2737958  -0.06289053\n",
      "  0.          0.        ], Reward: -1.7761463057576339, Done: False\n",
      "Step taken: 3, New state: [-0.5041291   0.2616171  -0.31481826 -1.277845   -0.27893066 -0.10269669\n",
      "  0.          0.        ], Reward: -2.005672253218306, Done: False\n",
      "Step taken: 2, New state: [-0.50720465  0.23330763 -0.30224425 -1.2592227  -0.28438702 -0.10912716\n",
      "  0.          0.        ], Reward: 2.228846759873119, Done: False\n",
      "Step taken: 3, New state: [-0.5101937   0.2043676  -0.29138842 -1.2877065  -0.2921225  -0.15470986\n",
      "  0.          0.        ], Reward: -2.462298495670835, Done: False\n",
      "Step taken: 1, New state: [-0.5132748   0.17486222 -0.30296895 -1.3123888  -0.29742116 -0.10597284\n",
      "  0.          0.        ], Reward: -2.4880865115883624, Done: False\n",
      "Step taken: 2, New state: [-0.5160591   0.14581175 -0.27365905 -1.2921162  -0.3023616  -0.09880914\n",
      "  0.          0.        ], Reward: 2.4167662361739533, Done: False\n",
      "Step taken: 0, New state: [-0.51884353  0.11616161 -0.2736599  -1.3187847  -0.30730206 -0.09880895\n",
      "  0.          0.        ], Reward: -2.6466706434768525, Done: False\n",
      "Step taken: 3, New state: [-0.5215512   0.08587683 -0.2640072  -1.3474405  -0.31431234 -0.14020512\n",
      "  0.          0.        ], Reward: -3.037802878788311, Done: False\n",
      "Step taken: 3, New state: [-0.52418983  0.05496813 -0.25537562 -1.3755816  -0.32313842 -0.17652172\n",
      "  0.          0.        ], Reward: -3.3641347401241988, Done: False\n",
      "Step taken: 0, New state: [-0.5268283   0.02346054 -0.25537863 -1.4022545  -0.33196446 -0.17652075\n",
      "  0.          0.        ], Reward: -3.5346084581377113, Done: False\n",
      "Step taken: 0, New state: [-0.5294665  -0.00864611 -0.25538176 -1.4289272  -0.34079045 -0.17651984\n",
      "  0.          0.        ], Reward: -3.726220146928142, Done: False\n",
      "Step taken: 2, New state: [-0.5368578  -0.03068017 -0.72475946 -0.9826824  -0.3638976  -0.47231263\n",
      "  1.          0.        ], Reward: 29.622434620565468, Done: False\n",
      "Step taken: 0, New state: [-0.5438651  -0.05292527 -0.697618   -0.98947537 -0.3674883  -0.07219408\n",
      "  0.          0.        ], Reward: -10.192452184166342, Done: False\n",
      "Step taken: 1, New state: [-0.55086887 -0.07560596 -0.703805   -1.007155   -0.36386132  0.07254452\n",
      "  1.          0.        ], Reward: 7.57029254807722, Done: False\n",
      "Step taken: 0, New state: [-0.558041   -0.09829719 -0.73117614 -1.0044819  -0.34367824  0.41022238\n",
      "  1.          0.        ], Reward: -0.41346091835941934, Done: False\n",
      "Step taken: 3, New state: [-0.56491    -0.11788122 -0.80679655 -0.86437064 -0.20221496  2.5393724\n",
      "  1.          0.        ], Reward: 19.07405828082534, Done: False\n",
      "Step taken: 2, New state: [-0.5722412  -0.1334421  -0.9329891  -0.6300418  -0.02942745  3.6826782\n",
      "  1.          0.        ], Reward: -100, Done: True\n",
      "Episode finished, score: -98.25112496788375\n",
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=0, agent=None, episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZLUDA device successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from DQN import DQNAgent\n",
    "lunar = LunarLanderEnv(render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork:\n",
      " DQN(\n",
      "  (input_layer1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (layer1_layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer2_output): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Starting training (code 781220)...\n",
      "[2025-06-04T19:37:49] Training DQN agent with parameters:\n",
      "  - gamma: 0.99\n",
      "  - epsilon: 1.0\n",
      "  - epsilon_decay: 0.995\n",
      "  - epsilon_min: 0.01\n",
      "  - learning_rate: 0.001\n",
      "  - batch_size: 64\n",
      "  - episodes: 1500\n",
      "  - target_network_update_freq: 10\n",
      "  - replays_per_episode: 1000\n",
      "\n",
      "[2025-06-04T19:37:49] Episode 1 (0.07%) had score: -102.03\n",
      "[2025-06-04T19:37:49]     Saving model to training/training_781220/episode_1_(-102.03).h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(lunar)\n\u001b[1;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\miniluz\\Uni\\aprendizaje-modulo-lunar\\DQN.py:324\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, save_path, score_window_size, backup_interval)\u001b[0m\n\u001b[0;32m    322\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 324\u001b[0m     _next_state, reward, done, _action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    326\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\miniluz\\Uni\\aprendizaje-modulo-lunar\\DQN.py:165\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Choose the action with the highest q(s,a)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(result)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# see training.ipynb for training the expanded agent\n",
    "# this won't work very well because we're doing it after fixing the epsilon decay\n",
    "# and as the paper shows that (somehow) makes results worse\n",
    "agent = DQNAgent(lunar)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork:\n",
      " DQN(\n",
      "  (input_layer1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (layer1_layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer2_output): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Prueba el agente sin ampliaciones\n",
    "base_agent = DQNAgent(lunar, epsilon=0.0)\n",
    "base_agent.load_model(\"training/v2_1500_dqn/episode_1451_(245.40).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished, score: 254.63741953606703\n",
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta una prueba pausable a mano\n",
    "test_lunar_lander(steps_to_run_before_pause=25, agent=base_agent, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished, score: 251.85484389671342\n",
      "Episode finished, score: 240.03597861884347\n",
      "Episode finished, score: 278.87584749660215\n",
      "Episode finished, score: 272.5421458821619\n",
      "Episode finished, score: -0.6000451496159229\n",
      "Episode finished, score: 6.836442651878514\n",
      "Episode finished, score: 266.6539839671116\n",
      "Episode finished, score: 142.36451823501622\n",
      "Episode finished, score: 293.5725201486866\n",
      "Episode finished, score: 154.07128402924832\n",
      "Episode finished, score: 282.26182906609677\n",
      "Episode finished, score: 251.78101622003157\n",
      "Episode finished, score: 264.63615338516456\n",
      "Episode finished, score: 247.3977245118059\n",
      "Episode finished, score: -31.207614755165196\n",
      "Episode finished, score: -8.560131028144326\n",
      "Episode finished, score: 248.3555843895607\n",
      "Episode finished, score: -41.75775764412475\n",
      "Episode finished, score: 225.7533712336732\n",
      "Episode finished, score: 279.10977099679474\n",
      "Episode finished, score: 254.82822842643284\n",
      "Episode finished, score: -16.007515295671567\n",
      "Episode finished, score: -16.87151401609799\n",
      "Episode finished, score: 257.90925312794104\n",
      "Episode finished, score: -39.43957804402868\n",
      "Episode finished, score: 281.3795094899977\n",
      "Episode finished, score: 238.12933661216812\n",
      "Episode finished, score: 248.75552969887525\n",
      "Episode finished, score: 296.6985358796188\n",
      "Episode finished, score: 136.56057070041922\n",
      "Episode finished, score: 278.4406347376362\n",
      "Episode finished, score: 135.51309667877499\n",
      "Episode finished, score: 285.22065634976536\n",
      "Episode finished, score: 250.17283172318466\n",
      "Episode finished, score: 139.92715492117892\n",
      "Episode finished, score: 245.6535469098705\n",
      "Episode finished, score: 242.7688292877768\n",
      "Episode finished, score: 273.10319700899265\n",
      "Episode finished, score: 277.6681780644134\n",
      "Episode finished, score: 256.14406535699663\n",
      "Episode finished, score: 258.67784994995236\n",
      "Episode finished, score: 22.774093755014178\n",
      "Episode finished, score: 250.33565881393298\n",
      "Episode finished, score: 174.7145642908613\n",
      "Episode finished, score: 269.2101753306305\n",
      "Episode finished, score: 253.06271288703635\n",
      "Episode finished, score: 273.34706651176293\n",
      "Episode finished, score: 288.0282117359755\n",
      "Episode finished, score: -155.12202969965347\n",
      "Episode finished, score: 150.21469270498446\n",
      "Episode finished, score: 260.1586213673984\n",
      "Episode finished, score: 34.386967017402185\n",
      "Episode finished, score: 16.119601390126377\n",
      "Episode finished, score: 258.3787795154288\n",
      "Episode finished, score: 303.7568680109713\n",
      "Episode finished, score: 280.64999306276525\n",
      "Episode finished, score: 280.6409960812709\n",
      "Episode finished, score: -107.76860291251555\n",
      "Episode finished, score: 263.832706241981\n",
      "Episode finished, score: 281.81420608565514\n",
      "Episode finished, score: 275.8976130454863\n",
      "Episode finished, score: 274.25693619223193\n",
      "Episode finished, score: 272.17495270300435\n",
      "Episode finished, score: 292.70684237640114\n",
      "Episode finished, score: 253.28901314059695\n",
      "Episode finished, score: 242.5106632908215\n",
      "Episode finished, score: 87.03422168448299\n",
      "Episode finished, score: 264.01456117408367\n",
      "Episode finished, score: -30.037387793934144\n",
      "Episode finished, score: 254.85392269350962\n",
      "Episode finished, score: 269.0515016218689\n",
      "Episode finished, score: 235.27182774368148\n",
      "Episode finished, score: 262.7027386226015\n",
      "Episode finished, score: -7.660293914605589\n",
      "Episode finished, score: 275.30026193453085\n",
      "Episode finished, score: 13.48063027555277\n",
      "Episode finished, score: 237.35308603017083\n",
      "Episode finished, score: 259.1538841416809\n",
      "Episode finished, score: 233.530023607041\n",
      "Episode finished, score: 266.0647360025059\n",
      "Episode finished, score: 287.4599659418443\n",
      "Episode finished, score: 300.5445090938414\n",
      "Episode finished, score: 273.85429118615616\n",
      "Episode finished, score: 259.510655867158\n",
      "Episode finished, score: -19.559278658252722\n",
      "Episode finished, score: 253.02361249556614\n",
      "Episode finished, score: 238.60905050234402\n",
      "Episode finished, score: 262.8259679908575\n",
      "Episode finished, score: 304.9622752156599\n",
      "Episode finished, score: 287.24287465439124\n",
      "Episode finished, score: 241.24131915156278\n",
      "Episode finished, score: 44.61740906674879\n",
      "Episode finished, score: 269.7639467688111\n",
      "Episode finished, score: 279.3685277459357\n",
      "Episode finished, score: 263.9194090028544\n",
      "Episode finished, score: 263.80386835749374\n",
      "Episode finished, score: 234.07280084454067\n",
      "Episode finished, score: 263.97521070765583\n",
      "Episode finished, score: 280.13929874320536\n",
      "Episode finished, score: 268.9664214399693\n",
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta 100 pruebas sin pausa\n",
    "test_lunar_lander(steps_to_run_before_pause=0, agent=base_agent, episodes=100, render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork:\n",
      " DQN(\n",
      "  (input_layer1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (layer1_layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer2_output): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Prueba el agente con ampliaciones\n",
    "extended_agent = DQNAgent(lunar, epsilon=0.0)\n",
    "extended_agent.load_model(\"training/v3_dqn_dqps_1500_0.001_epsilon_arreglado/episode_1451_(256.82).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ejecuta una prueba pausable a mano\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtest_lunar_lander\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps_to_run_before_pause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m, in \u001b[0;36mtest_lunar_lander\u001b[1;34m(steps_to_run_before_pause, agent, episodes, render_mode)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps_to_run_before_pause \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m counter \u001b[38;5;241m%\u001b[39m steps_to_run_before_pause \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 24\u001b[0m         \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPress Enter to continue...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     27\u001b[0m         _, reward, done, action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact()\n",
      "File \u001b[1;32mc:\\Users\\miniluz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\aprendizaje-modulo-lunar-5laruUkD-py3.10\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\miniluz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\aprendizaje-modulo-lunar-5laruUkD-py3.10\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Ejecuta una prueba pausable a mano\n",
    "test_lunar_lander(steps_to_run_before_pause=25, agent=extended_agent, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished, score: 263.50983757504935\n",
      "Episode finished, score: 158.060714681064\n",
      "Episode finished, score: 266.6954232111334\n",
      "Episode finished, score: 249.7159098584065\n",
      "Episode finished, score: 132.2918362436529\n",
      "Episode finished, score: 279.01980327602996\n",
      "Episode finished, score: 277.8106491178089\n",
      "Episode finished, score: 0.39295440298266726\n",
      "Episode finished, score: 236.12815574729092\n",
      "Episode finished, score: 233.8347830973121\n",
      "Episode finished, score: 253.88567952030846\n",
      "Episode finished, score: 256.2900969061733\n",
      "Episode finished, score: 281.7311042733828\n",
      "Episode finished, score: 272.00297362767685\n",
      "Episode finished, score: 255.09219517984266\n",
      "Episode finished, score: 271.7088281107303\n",
      "Episode finished, score: 228.1977630012243\n",
      "Episode finished, score: 283.2458860690462\n",
      "Episode finished, score: 224.8355108175611\n",
      "Episode finished, score: 274.8685157413163\n",
      "Episode finished, score: 282.72283820922235\n",
      "Episode finished, score: -5.657910163132414\n",
      "Episode finished, score: 268.91544721350766\n",
      "Episode finished, score: 267.04390201920876\n",
      "Episode finished, score: 264.6750873160854\n",
      "Episode finished, score: 254.21091184938177\n",
      "Episode finished, score: 28.60815575727173\n",
      "Episode finished, score: 251.69089911741676\n",
      "Episode finished, score: 240.5233276846315\n",
      "Episode finished, score: 259.0891922396034\n",
      "Episode finished, score: 242.22782374225505\n",
      "Episode finished, score: 262.74223890653803\n",
      "Episode finished, score: 289.80087514676177\n",
      "Episode finished, score: 144.05692129581857\n",
      "Episode finished, score: 267.49759383990624\n",
      "Episode finished, score: 263.9099145438055\n",
      "Episode finished, score: 252.04999673828564\n",
      "Episode finished, score: 46.86834155497206\n",
      "Episode finished, score: 298.0227406404583\n",
      "Episode finished, score: 36.24093896475782\n",
      "Episode finished, score: 140.14941819112593\n",
      "Episode finished, score: 275.0752525204269\n",
      "Episode finished, score: 155.67408962413205\n",
      "Episode finished, score: 256.6676676135031\n",
      "Episode finished, score: 264.6739148015988\n",
      "Episode finished, score: 270.8769728514759\n",
      "Episode finished, score: 272.83325537886725\n",
      "Episode finished, score: 278.0254980765094\n",
      "Episode finished, score: 262.30476696788224\n",
      "Episode finished, score: 274.011981823922\n",
      "Episode finished, score: 275.51462268392737\n",
      "Episode finished, score: 237.6111733176696\n",
      "Episode finished, score: 261.69033019233245\n",
      "Episode finished, score: 256.5417392192269\n",
      "Episode finished, score: 260.92902020010627\n",
      "Episode finished, score: 265.66975813174736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ejecuta 100 pruebas sin pausa\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtest_lunar_lander\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps_to_run_before_pause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 27\u001b[0m, in \u001b[0;36mtest_lunar_lander\u001b[1;34m(steps_to_run_before_pause, agent, episodes, render_mode)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPress Enter to continue...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 27\u001b[0m     _, reward, done, action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Sample a random action from the action space\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m lunar\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\miniluz\\Uni\\aprendizaje-modulo-lunar\\DQN.py:162\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# With probability 1 - epsilon\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Use q-network to evaluate q(s,a) for all actions\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 162\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    165\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network(state_tensor)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ejecuta 100 pruebas sin pausa\n",
    "test_lunar_lander(steps_to_run_before_pause=0, agent=extended_agent, episodes=100, render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE import REINFORCEAgent\n",
    "lunar = LunarLanderEnv(render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = REINFORCEAgent(lunar, episodes=5000)\n",
    "# agent.load_model(\"modelo_REINFORCE.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Discovirtual-us\\IA personal\\Aprendizaje por refuerzo trabajo 2024-2025\\VersionAlumnos\\REINFORCE.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.actor_net.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "agent = REINFORCEAgent(lunar)\n",
    "agent.load_model(\"modelo_REINFORCE.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished, score: 264.42781863081314\n",
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=75, agent=agent, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aprendizaje-modulo-lunar-5laruUkD-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
